{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Detect fraudulent emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the s3 path of the input data file\n",
    "s3_source_file=\"s3://bucket/mycsv.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the s3 path of the output data file\n",
    "bucket = 'aegovansagemaker' # Replace with your own bucket name if needed, this is the destination bucket\n",
    "prefix = 'emailfraud/sagemaker' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. If you don't specify a bucket, SageMaker SDK will create a default bucket following a pre-defined naming convention in the same region. \n",
    "- The IAM role ARN used to give SageMaker access to your data. It can be fetched using the **get_execution_role** method from sagemaker python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-plot==0.3.7\n",
    "!pip install numpy==1.15.1\n",
    "!pip install matplotlib==2.7.3\n",
    "!pip install scikit-learn==0.20.0\n",
    "!pip install nltk==3.3\n",
    "!pip install pandas==0.22.0\n",
    "!pip install sagemaker==1.10.1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_tmp_dir=\"tmp\"\n",
    "inputfile=\"{}/data.csv\".format(local_tmp_dir)\n",
    "!mkdir -p local_tmp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!aws s3 cp $s3_source_file $inputfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head $inputfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the file as expected by blazing text with no header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "\n",
    "\n",
    "def get_data(inputfile):\n",
    "    nltk.download('punkt')\n",
    "    data = []\n",
    "    with open(inputfile, \"r\") as i:\n",
    "        csv_reader = csv.reader(i, delimiter=\",\", quotechar='\"')\n",
    "        # Ignore head\n",
    "        next(csv_reader)\n",
    "        for l in csv_reader:\n",
    "            raw_label= l[5]\n",
    "            label=\"__label__{}\".format(raw_label)\n",
    "            # Features - Comments, Make, Model , Year, Price\n",
    "            text = \"{} {} {} {} {}\".format(l[4], l[11], l[12], l[13], l[14]).replace(\"\\n\", \"\")\n",
    "            tokens = nltk.word_tokenize(text.lower())      \n",
    "            data.append({\"label\" : raw_label, \"text\":text, \"tokens\": tokens})\n",
    "\n",
    "    return data\n",
    "        \n",
    "\n",
    "\n",
    "   \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_list = get_data(inputfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "data = pd.DataFrame(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 10000)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total no of records in the dataset {}\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the dataset and the classes to get some understanding about how the data and the label is provided in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"label\"].value_counts().plot.pie(autopct='%.2f',figsize=(5, 5), colors=[\"green\",\"yellow\"])\n",
    "plt.title = \"Fraud class distribution\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = data[\"label\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into Train, Test and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "train_test, validation = train_test_split(data, test_size = 0.2, random_state = 777)\n",
    "train, test = train_test_split(train_test, test_size = 0.2, random_state = 777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the class distribution for the train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( 1,3, figsize=(15,5))\n",
    "axis=ax[0]\n",
    "train[\"label\"].value_counts().plot.pie(autopct='%.2f', colors=[\"green\",\"yellow\"], ax=axis)\n",
    "axis.set_title(\"Train set fraud class distribution\")\n",
    "\n",
    "axis=ax[1]\n",
    "validation[\"label\"].value_counts().plot.pie(autopct='%.2f', colors=[\"green\",\"yellow\"], ax=axis)\n",
    "axis.set_title(\"Validation set fraud class distribution\")\n",
    "\n",
    "\n",
    "axis=ax[2]\n",
    "test[\"label\"].value_counts().plot.pie(autopct='%.2f', colors=[\"green\",\"yellow\"], ax=axis)\n",
    "axis.set_title(\"Test set fraud class distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the dataset file for Blazing text\n",
    "The first column should be the label formatted as \\__label__<labelname\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_formatted_file(data, outputfile):\n",
    "    with open(outputfile , \"w\") as o:\n",
    "        for index, row in data.iterrows():\n",
    "            label=\"__label__{}\".format(row[\"label\"])\n",
    "            token_sep_space=\" \".join(row[\"tokens\"])\n",
    "            o.write(\"{} {}\\n\".format(label, token_sep_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "\n",
    "\n",
    "formatted_train_file=\"{}/train.txt\".format(local_tmp_dir)\n",
    "formatted_validation_file=\"{}/validation.txt\".format(local_tmp_dir)\n",
    "formatted_test_file=\"{}/test.txt\".format(local_tmp_dir)\n",
    "\n",
    "\n",
    "\n",
    "write_formatted_file(train, formatted_train_file)\n",
    "write_formatted_file(validation, formatted_validation_file)\n",
    "write_formatted_file(test, formatted_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the formatted file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head $formatted_train_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preprocessing cell might take a minute to run. After the data preprocessing is complete, we need to upload it to S3 so that it can be consumed by SageMaker to execute training jobs. We'll use Python SDK to upload these two files to the bucket and prefix location that we have set above.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path=formatted_train_file, bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path=formatted_validation_file, bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'blazingtext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the BlazingText model for supervised text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.4xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to [algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html) for the complete list of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=600,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.01,\n",
    "                            vector_dim=150,\n",
    "                            early_stopping=True,\n",
    "                            patience=50,\n",
    "                            min_epochs=5,\n",
    "                            subwords=False,\n",
    "                            word_ngrams=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyper-parameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trainingjobname, container_name, role):\n",
    "    import boto3\n",
    "    from time import gmtime, strftime\n",
    "\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    model_name=\"blazingtextemailfraud\" + '-model-' +  strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "    print(model_name)\n",
    "    desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=trainingjobname)\n",
    "    model_data = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "    print(model_data)\n",
    "\n",
    "    primary_container = {\n",
    "        'Image': container_name,\n",
    "        'ModelDataUrl': model_data\n",
    "    }\n",
    "\n",
    "    client = boto3.client('sagemaker')\n",
    "    create_model_response = client.create_model(\n",
    "        ModelName = model_name,\n",
    "        ExecutionRoleArn = role,\n",
    "        PrimaryContainer = primary_container)\n",
    "    print(create_model_response['ModelArn'])\n",
    "    print( model_name)\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "chainer_training_job = bt_model.latest_training_job.name\n",
    "model_name = create_model(chainer_training_job, container, role)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamter Tuning to optimise even further.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": {\n",
    "      \"CategoricalParameterRanges\": [],\n",
    "      \"ContinuousParameterRanges\": [\n",
    "       \n",
    "        {\n",
    "          \"MaxValue\": \".08\",\n",
    "          \"MinValue\": \".01\",\n",
    "          \"Name\": \"learning_rate\"\n",
    "        }\n",
    "      ],\n",
    "      \"IntegerParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"1000\",\n",
    "          \"MinValue\": \"200\",\n",
    "          \"Name\": \"epochs\"\n",
    "        },\n",
    "          \n",
    "        {\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"2\",\n",
    "          \"Name\": \"min_count\"\n",
    "        },\n",
    "           {\n",
    "          \"MaxValue\": \"400\",\n",
    "          \"MinValue\": \"50\",\n",
    "          \"Name\": \"vector_dim\"\n",
    "        },\n",
    "          \n",
    "      ]\n",
    "    },\n",
    "    \"ResourceLimits\": {\n",
    "      \"MaxNumberOfTrainingJobs\": 20,\n",
    "      \"MaxParallelTrainingJobs\": 3\n",
    "    },\n",
    "    \"Strategy\": \"Bayesian\",\n",
    "    \"HyperParameterTuningJobObjective\": {\n",
    "      \"MetricName\": \"validation:accuracy\",\n",
    "      \"Type\": \"Maximize\"\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "     \n",
    "training_job_definition = {\n",
    "    \"AlgorithmSpecification\": {\n",
    "      \"TrainingImage\": container,\n",
    "      \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "      {\n",
    "        \"ChannelName\": \"train\",\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"ContentType\": \"text/plain\",\n",
    "        \"DataSource\": {\n",
    "          \"S3DataSource\": {\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": s3_train_data\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"ChannelName\": \"validation\",\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"ContentType\": \"text/plain\",\n",
    "        \"DataSource\": {\n",
    "          \"S3DataSource\": {\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": s3_validation_data\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "      \"S3OutputPath\": \"s3://{}/{}/output\".format(bucket,prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "      \"InstanceCount\": 2,\n",
    "      \"InstanceType\": \"ml.c4.4xlarge\",\n",
    "      \"VolumeSizeInGB\": 10\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"StaticHyperParameters\": {\n",
    "        \"mode\":\"supervised\",\n",
    "        \"early_stopping\":\"True\",\n",
    "        \"patience\":\"50\",\n",
    "        \"min_epochs\":\"5\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "      \"MaxRuntimeInSeconds\": 43200\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "tuning_job_name = \"FraudEnquiry-\" + strftime(\"%Y%m%d%H%M%S\", gmtime())\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "\n",
    "smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\n",
    "                                           HyperParameterTuningJobConfig = tuning_job_config,\n",
    "                                           TrainingJobDefinition = training_job_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the best training job and create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training job is blazingtextemail-20181015053131-019-9d381ee2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "\n",
    "tuning_job_details = smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)\n",
    "tuningjob_status = tuning_job_details[\"HyperParameterTuningJobStatus\"]\n",
    "\n",
    "if tuningjob_status == \"Completed\":\n",
    "    best_training_job_name = tuning_job_details[\"BestTrainingJob\"][\"TrainingJobName\"]\n",
    "    print(\"The best training job is {}\".format(best_training_job_name))\n",
    "else:\n",
    "    print(\"The hyperparamter tuning job is in status {}. If the job is inprogress wait until it completes sucessfully ..  \". format(tuningjob_status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blazingtextemailfraud-model-2018-10-18-23-03-12\n",
      "s3://aegovansagemaker/cs_emailfraud/sagemaker/output/blazingtextemail-20181015053131-019-9d381ee2/output/model.tar.gz\n",
      "arn:aws:sagemaker:us-east-2:324346001917:model/blazingtextemailfraud-model-2018-10-18-23-03-12\n",
      "blazingtextemailfraud-model-2018-10-18-23-03-12\n"
     ]
    }
   ],
   "source": [
    "model_name = create_model(best_training_job_name, container, role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using sagemaker batch transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchfileinput=\"{}/batchtest.json\".format(local_tmp_dir)\n",
    "batchfileoutput=\"{}/batchtest.output.json\".format(local_tmp_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a json file that is formatted  for blazing text inference\n",
    "It kind of looks like\n",
    "```json\n",
    "{\n",
    "  \"instances\": [\"hey mate , very interested in this\", \"does it include a pakc ?\", \"hi is it working\"],\n",
    "  \"configuration\": {\n",
    "    \"k\": 2\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "from itertools import islice\n",
    "import math\n",
    "import struct\n",
    "import json\n",
    "\n",
    "test_data = test\n",
    "\n",
    "input_records = [\" \".join(l[\"tokens\"]) for index, l in test_data.iterrows()]\n",
    "labels = [\"__label__{}\".format(l[\"label\"]) for index, l  in test_data.iterrows() ]\n",
    "\n",
    "input_json = {\"instances\" : input_records, \"configuration\": {\"k\": num_classes}}\n",
    "with open(batchfileinput , \"w\") as f:\n",
    "    f.write(json.dumps(input_json))\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "\n",
    "fmttime= strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "input_key_file=\"batchvalidation.csv\"\n",
    "input_batch_key=\"{}/batchTransform/{}_input/{}\".format(prefix, fmttime, input_key_file)\n",
    "input_location = 's3://{}/{}'.format(bucket, input_batch_key)\n",
    "output_batch_key = \"{}/batchTransform/{}_output\".format(prefix,fmttime)\n",
    "output_location = 's3://{}/{}'.format(bucket, output_batch_key)\n",
    "\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file(batchfileinput, bucket, input_batch_key)\n",
    "\n",
    "# Initialize the transformer object\n",
    "transformer =sagemaker.transformer.Transformer(\n",
    "    base_transform_job_name='Batch-Transform',\n",
    "    model_name=model_name,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c4.xlarge',\n",
    "    output_path=output_location\n",
    "    )\n",
    "# To start a transform job:\n",
    "transformer.transform(input_location, content_type='application/json')\n",
    "# Then wait until transform job is completed\n",
    "transformer.wait()\n",
    "\n",
    "# To fetch validation result \n",
    "outputkey ='{}/{}.out'.format(output_batch_key, input_key_file)\n",
    "print(outputkey)\n",
    "s3_client.download_file(bucket, outputkey, batchfileoutput)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(batchfileoutput) as f:\n",
    "    predicted = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of Blazing text inference looks like this\n",
    "```json\n",
    "[{\n",
    "    'prob': [0.9910072088241577, 0.00901278480887413],\n",
    "    'label': ['__label__0', '__label__1']\n",
    "  }, {\n",
    "    'prob': [0.9973444938659668, 0.002675538882613182],\n",
    "    'label': ['__label__1', '__label__0']\n",
    "  }, {\n",
    "    'prob': [0.9969576001167297, 0.003062397940084338],\n",
    "    'label': ['__label__0', '__label__1']\n",
    "  ]\n",
    "]\n",
    "```\n",
    "\n",
    "So lets format it so that we can analyse the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = [ r['label'][r['prob'].index(max(r['prob']))] for r in predicted]\n",
    "predicted_confidence_score = [ max(r['prob'])/sum(r['prob']) for r in predicted]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the label format \\__label__1 to 1 , as the scores work on numeric labels only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y_actual = pd.DataFrame(labels, columns = [\"label\"]) #== \"__label__1\"\n",
    "y_pred = pd.DataFrame(predicted_labels, columns = [\"predicted_label\"]) #== \"__label__1\"\n",
    "y_confidence_score =  pd.DataFrame(predicted_confidence_score, columns = [\"predicted_label_confidence_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "unique_classes = predicted[0]['label']\n",
    "\n",
    "y_classwise_prob = pd.DataFrame(0, index=np.arange(len(predicted)), columns=unique_classes)\n",
    "for c in unique_classes:\n",
    "    y_classwise_prob[c] = pd.DataFrame([ r['prob'][r['label'].index(c)]/sum(r['prob']) for r in predicted])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_label=\"__label__1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_display= pd.DataFrame()\n",
    "y_display[\"predicted_label\"] = y_pred[\"predicted_label\"]\n",
    "y_display['confidence_score'] = y_confidence_score[\"predicted_label_confidence_score\"]\n",
    "y_display['score_postive_class'] = y_classwise_prob[\"__label__1\"]\n",
    "y_display['actual'] = y_actual['label']\n",
    "y_display.head(n=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_actual, y_pred, labels=None, true_labels=None, pred_labels=None, title=None, normalize=False, hide_zeros=False, hide_counts=False, x_tick_rotation=0, ax=None, figsize=None, cmap='Blues', title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "## Precision recall\n",
    "macro_score = sklearn.metrics.average_precision_score( y_actual, y_classwise_prob[positive_label] ,  pos_label = \"__label__1\")\n",
    "f1score = sklearn.metrics.f1_score(y_actual, y_pred , pos_label = positive_label, average='binary')\n",
    "precision = sklearn.metrics.precision_score(y_actual, y_pred ,  pos_label = positive_label, average='binary')\n",
    "recall = sklearn.metrics.recall_score( y_actual, y_pred ,  pos_label = positive_label, average='binary')\n",
    "\n",
    "## Unweighted ROC\n",
    "acc = accuracy_score(y_actual, y_pred)\n",
    "fpr, tpr, _ = roc_curve(y_actual,y_classwise_prob[positive_label], pos_label=positive_label)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(\"The AUC under precision recall curve is {}\".format( macro_score))\n",
    "print(\"F1 score {}\".format(f1score))\n",
    "print(\"Precision score {}\".format(precision))\n",
    "print(\"Recall score {}\".format(recall))\n",
    "print(\"Accuracy {}\".format( acc))\n",
    "print(\"Area Under ROC curve {}\".format( roc_auc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true =  y_actual# ground truth labels\n",
    "skplt.metrics.plot_roc(y_true, y_classwise_prob, figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the endpoint\n",
    "Now that the results look satisfactory, deploy the inference endpoint to serve inference over http"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the endpoint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FraudDetectionConfig-2018-10-18-23-25-01\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-2:324346001917:endpoint-config/frauddetectionconfig-2018-10-18-23-25-01\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "\n",
    "endpoint_config_name = 'FraudDetectionConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = smclient.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create endpoint\n",
    "This takes 6-7 minutes to complete.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "import time\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "\n",
    "\n",
    "endpoint_name = 'FraudDetection-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = smclient.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = smclient.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = smclient.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Sample invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "def isFraud(json_payload):\n",
    "    content_type = \"application/json\"\n",
    "    payload = json.dumps(json_payload)\n",
    "    response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType=content_type, \n",
    "                                   Body=payload)\n",
    "    return json.loads(response[\"Body\"].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'prob': [0.9939164519309998, 0.0061035798862576485], 'label': ['__label__0', '__label__1']}, {'prob': [1.0000100135803223, 1.0000003385357559e-05], 'label': ['__label__0', '__label__1']}]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentences = [\"this is a .\",\n",
    "            \"Can we organise a time to view please.\"]\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [' '.join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\" : tokenized_sentences, \"configuration\": {\"k\": 2}}\n",
    "\n",
    "result = isFraud(payload)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo cleanup\n",
    "Since this is a demo delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '7f298191-2130-46b5-ba4c-f9fe8d99252a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '7f298191-2130-46b5-ba4c-f9fe8d99252a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 18 Oct 2018 23:37:28 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smclient = boto3.Session().client('sagemaker')\n",
    "smclient.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
